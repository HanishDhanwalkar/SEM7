{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 a\n",
    "\n",
    "<img src= '210100060_Q1a .jpeg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary modules\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solver(X,Y,d,n):\n",
    "    # Decision Variables\n",
    "    w = cp.Variable(d)\n",
    "    b = cp.Variable()\n",
    "    gamma = cp.Variable(nonneg=True)\n",
    "\n",
    "    # Constraints\n",
    "    constraints = [Y[i] * (X[i] @ w - b) >= gamma for i in range(n)]\n",
    "\n",
    "    # Objective function\n",
    "    objective = cp.Minimize(0)\n",
    "\n",
    "    # Formulating the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Solving the problem\n",
    "    problem.solve()\n",
    "\n",
    "    # Printing the results\n",
    "    if problem.status == cp.OPTIMAL and gamma.value!=0:\n",
    "        print(\"The dataset is linearly separable.\")\n",
    "        print(f\"Optimal weights (w): {w.value}\")\n",
    "        print(f\"Optimal bias (b): {b.value}\")\n",
    "        print(f\"Margin (gamma): {gamma.value}\")\n",
    "    else:\n",
    "        print(\"The dataset is not linearly separable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is linearly separable.\n",
      "Optimal weights (w): [-0.07235884 -0.90799697  1.06027588  1.40766531]\n",
      "Optimal bias (b): 0.7416443368130958\n",
      "Margin (gamma): 0.40539791994425406\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset \n",
    "data1=pd.read_csv('data1.csv',header=None)\n",
    "d=4\n",
    "\n",
    "# Defining the features and labels\n",
    "X1=data1.iloc[:,0:d].values\n",
    "Y=data1.iloc[:,d:].values\n",
    "n=Y.shape[0]\n",
    "\n",
    "\n",
    "# Ensuring the labels are 1,-1\n",
    "a=np.min(Y)\n",
    "Y1=np.where(Y==a,-1,1)\n",
    "\n",
    "# Solving for checking linear separablity\n",
    "Solver(X1,Y1,d,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is linearly separable.\n",
      "Optimal weights (w): [-0.17256734 -0.49340767  0.98393321  0.78370697]\n",
      "Optimal bias (b): 1.4832879835455905\n",
      "Margin (gamma): 0.5816990822838657\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset \n",
    "data2=pd.read_csv('data2.csv',header=None)\n",
    "d=4\n",
    "\n",
    "# Defining the features and labels\n",
    "X2=data2.iloc[:,0:d].values\n",
    "Y=data2.iloc[:,d:].values\n",
    "n=Y.shape[0]\n",
    "\n",
    "# Ensuring the labels are 1,-1\n",
    "a=np.min(Y)\n",
    "Y2=np.where(Y==a,-1,1)\n",
    "\n",
    "# Solving for checking linear separablity\n",
    "Solver(X2,Y2,d,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is not linearly separable.\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset \n",
    "data3=pd.read_csv('data3.csv',header=None)\n",
    "d=4\n",
    "\n",
    "# Defining the features and labels\n",
    "X3=data3.iloc[:,0:d].values\n",
    "Y=data3.iloc[:,d:].values\n",
    "n=Y.shape[0]\n",
    "\n",
    "# Ensuring the labels are 1,-1\n",
    "a=np.min(Y)\n",
    "Y3=np.where(Y==a,-1,1)\n",
    "\n",
    "\n",
    "# Solving for checking linear separablity\n",
    "Solver(X3,Y3,d,n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the prediction for the example x using weight w\n",
    "def perceptron_prediction(w, x):\n",
    "    if np.matmul(w.T,x)>0:\n",
    "        prediction=1\n",
    "    else:\n",
    "        prediction=-1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement the update rule for updating weights in perceptron learning algorithm\n",
    "def perceptron_update_weights(w, x, y, y_pred):\n",
    "    is_mistake = False\n",
    "    if y!=y_pred:\n",
    "        w+=y*x\n",
    "        is_mistake=True\n",
    "    return w, is_mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X,Y):\n",
    "    # Initializing the weights\n",
    "    w = np.random.randn(5)\n",
    "    epochs=0 # Useful for executing multiple passes over a finite data set\n",
    "    num_mistakes = 99 # Some dummy value to get the while loop going\n",
    "    max_epochs = 100\n",
    "\n",
    "    while num_mistakes > 0 and epochs<max_epochs:   # until mistakes are not zero or number of epochs reach max_epochs\n",
    "        num_mistakes = 0\n",
    "        for i in range(len(Y)):\n",
    "            # Feature vector x from data set D\n",
    "            x =X[i,:]\n",
    "            \n",
    "            # Appending an additional constant feature 1 to x \n",
    "            x = np.concatenate([x,[1]])\n",
    "\n",
    "            y_hat = perceptron_prediction(w, x)\n",
    "\n",
    "            # Label y for x from data set D\n",
    "            y = Y[i]\n",
    "            \n",
    "            w, is_mistake = perceptron_update_weights(w, x, y, y_hat)\n",
    "            if is_mistake:\n",
    "              num_mistakes += 1\n",
    "        epochs=epochs+1\n",
    "    return w,num_mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is linearly separable.\n",
      "Optimal weights (w): [-1.35792889 -3.46786758  4.95692262  1.87975609 -1.24253215]\n"
     ]
    }
   ],
   "source": [
    "[w,num_mistakes]=train_perceptron(X1,Y1)\n",
    "if num_mistakes==0:\n",
    "    print(\"The dataset is linearly separable.\")\n",
    "    print(f\"Optimal weights (w): {w}\")\n",
    "else:\n",
    "    print(\"The dataset is not linearly separable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is linearly separable.\n",
      "Optimal weights (w): [-2.94251    -2.94325925  7.27874496  4.26799558 -1.01964203]\n"
     ]
    }
   ],
   "source": [
    "[w,num_mistakes]=train_perceptron(X2,Y2)\n",
    "if num_mistakes==0:\n",
    "    print(\"The dataset is linearly separable.\")\n",
    "    print(f\"Optimal weights (w): {w}\")\n",
    "else:\n",
    "    print(\"The dataset is not linearly separable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is not linearly separable.\n"
     ]
    }
   ],
   "source": [
    "[w,num_mistakes]=train_perceptron(X3,Y3)\n",
    "if num_mistakes==0:\n",
    "    print(\"The dataset is linearly separable.\")\n",
    "    print(f\"Optimal weights (w): {w}\")\n",
    "else:\n",
    "    print(\"The dataset is not linearly separable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is linearly separable, then by the mistake bound analysis, the number of mistakes is finite as we come closer to the optimal hyperplane and should become zero after a certain number of epochs. Thus by choosing a large number of epochs we check for convergence of the nmber of mistakes to 0, to determine separability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of checking for separatibilty,both the methods give the same answer. However, the weights are not exactly the same because in both the cases we are not tryng to find the optimal hyperplane, rather the feasible hyperplane which isn't unique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
