{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiYIneQcFsXA"
      },
      "source": [
        "ANSWERS:\n",
        "Q2 a)  \n",
        "\n",
        "    def linear(self, z):\n",
        "        \"\"\"returns result of the linear activation function on input z.\"\"\"\n",
        "        return z\n",
        "\n",
        "    def linearPrime(self,z):\n",
        "        \"\"\"returns derivative of linear activation function applied on input z.\"\"\"\n",
        "        return np.ones_like(dz)\n",
        "Q2 b)\n",
        "\n",
        "    def ReLU(self,z):\n",
        "        \"\"\"returns result of the ReLU activation function on input z.\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def ReLUPrime(self,z):\n",
        "        \"\"\"returns derivative of ReLU activation function applied on input z.\"\"\"\n",
        "        return (dz > 0).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcl9wZQdEih6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(1000)\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pA8Ke9MjLrw",
        "outputId": "3e25b4df-e778-490a-f656-0c3c7082a955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: X - (60000, 28, 28), y - (60000,)\n",
            "Test: X - (10000, 28, 28), y - (10000,)\n"
          ]
        }
      ],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "print(f'Train: X - {X_train.shape}, y - {Y_train.shape}')\n",
        "print(f'Test: X - {X_test.shape}, y - {Y_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "E9ogIbPkjRN0",
        "outputId": "b79592d2-e964-4bbc-9e10-9538ed759445"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Label in dataset: 4\n"
          ]
        }
      ],
      "source": [
        "plt.imshow(X_train[20],cmap='gray')\n",
        "plt.show()\n",
        "print(f\"\\n\\nLabel in dataset: {Y_train[20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4olvPpzrjnp2",
        "outputId": "585b83a0-2112-4f70-d3ee-3a43233ca4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "digits = np.unique(Y_train)\n",
        "print(digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INwTlURrjqWf",
        "outputId": "52927ef7-5dc4-4eb7-ef7d-7a325e52ac5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n"
          ]
        }
      ],
      "source": [
        "# M = 30 # no of instances for each class\n",
        "# fig, axs = plt.subplots(len(digits), M, figsize=(18,4))\n",
        "# for i,d in enumerate(digits):\n",
        "#     for j in range(M):\n",
        "#         axs[i,j].imshow(X_train[Y_train==d][j], cmap='gray')\n",
        "#         axs[i,j].axis('off')\n",
        "\n",
        "#counts for all unique class labels\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ziZ-Z8nkmVH",
        "outputId": "9988f299-627c-4109-86e7-5e7d53d4f447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes - Train: (60000, 784), Test: (10000, 784)\n",
            "Minimum pixel value = 0\n",
            "Maximum pixel value = 255\n",
            "Minimum pixel value = 0.0\n",
            "Maximum pixel value = 1.0\n"
          ]
        }
      ],
      "source": [
        "# Pre processing\n",
        "# flattening the input\n",
        "\n",
        "X_train_flattened = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flattened = X_test.reshape((X_test.shape[0], -1))\n",
        "X_normalized_train = np.round(X_train_flattened/X_train_flattened.max(), 3)\n",
        "X_normalized_test = np.round(X_test_flattened/X_test_flattened.max(), 3)\n",
        "\n",
        "print(f\"Shapes - Train: {X_train_flattened.shape}, Test: {X_test_flattened.shape}\")\n",
        "print(\"Minimum pixel value =\", X_train_flattened.min())\n",
        "print(\"Maximum pixel value =\", X_train_flattened.max())\n",
        "#print the new values\n",
        "print(\"Minimum pixel value =\", X_normalized_train.min())\n",
        "print(\"Maximum pixel value =\", X_normalized_train.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjvwqyWsk6Hy",
        "outputId": "e45cfa84-5f80-486e-9d7f-f54e5ab7d3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainset shapes - X: (12665, 784), Y: (12665,)\n",
            "Testset shapes - X: (2115, 784), Y: (2115,)\n",
            "\n",
            "Train statistics - 0: 5923, 1: 6742\n",
            "Test statistics - 0: 980, 1: 1135\n"
          ]
        }
      ],
      "source": [
        "#for train data\n",
        "train_idx = np.where((Y_train==0) | (Y_train==1))# Here we get the index of values 0 and 1\n",
        "X_train_bin = X_normalized_train[train_idx]\n",
        "Y_train_bin = Y_train[train_idx]\n",
        "\n",
        "#for test data\n",
        "test_idx = np.where((Y_test==0) | (Y_test==1))\n",
        "X_test_bin = X_normalized_test[test_idx]\n",
        "Y_test_bin = Y_test[test_idx]\n",
        "\n",
        "print(f'Trainset shapes - X: {X_train_bin.shape}, Y: {Y_train_bin.shape}')\n",
        "print(f'Testset shapes - X: {X_test_bin.shape}, Y: {Y_test_bin.shape}')\n",
        "\n",
        "print(f'\\nTrain statistics - 0: {(Y_train_bin==0).sum()}, 1: {(Y_train_bin==1).sum()}')\n",
        "print(f'Test statistics - 0: {(Y_test_bin==0).sum()}, 1: {(Y_test_bin==1).sum()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI3_LHq7lD2Q",
        "outputId": "f98ffa71-810f-4c71-8082-2148b962315b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instances in new train-set: 10132\n",
            "Instances in val-set: 2533\n",
            "\n",
            "Count of 0s and 1s in train-set: 4745 5387\n",
            "Count of 0s and 1s in val-set: 1178 1355\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_bin, X_val_bin, Y_train_bin, Y_val_bin = train_test_split(X_train_bin, Y_train_bin, test_size=0.2, random_state = 28)\n",
        "\n",
        "print(\"Instances in new train-set:\", len(Y_train_bin))\n",
        "print(\"Instances in val-set:\", len(Y_val_bin))\n",
        "print(\"\\nCount of 0s and 1s in train-set:\", (Y_train_bin==0).sum(), (Y_train_bin==1).sum())\n",
        "print(\"Count of 0s and 1s in val-set:\", (Y_val_bin==0).sum(), (Y_val_bin==1).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4ClJ8qblI4k",
        "outputId": "1b8942a8-2963-4afe-8d79-ec7b4e38cb4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes - ((784, 10132), (784, 2533), (784, 2115))\n"
          ]
        }
      ],
      "source": [
        "# Transpose feature matrices as per our model requirement\n",
        "\n",
        "X_train_bin, X_val_bin, X_test_bin = X_train_bin.T, X_val_bin.T, X_test_bin.T\n",
        "print(f'Shapes - {X_train_bin.shape, X_val_bin.shape, X_test_bin.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIoMJPr6CB4v"
      },
      "outputs": [],
      "source": [
        "class Neural_Network():\n",
        "    def __init__(self, neurons, Activations, initialization='randn'):\n",
        "        \"\"\"Define the NN design parameters\n",
        "        Args:\n",
        "            neurons (int array): list of number of neurons in each layer\n",
        "            Activations (str array): list of activations to be used for hidden and output layers\n",
        "            initialization (str, optional): Choose weight initialization from\n",
        "             uniform or normal distribution. Defaults to 'randn'.\n",
        "        \"\"\"\n",
        "\n",
        "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
        "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
        "        self.layers = len(neurons)\n",
        "        self.weights = [] # weights for each layer\n",
        "        self.biases = [] # biases in each layer\n",
        "        self.layer_activations = [] # activations for each layer\n",
        "\n",
        "        if initialization == 'rand':\n",
        "            self.initializer = np.random.rand\n",
        "        elif initialization == 'randn':\n",
        "            self.initializer = np.random.randn\n",
        "        else:\n",
        "           raise ValueError(\"initialization must be 'rand' or 'randn' or 'he' or 'xavier'\")\n",
        "        for i in range(len(neurons)-1):\n",
        "            self.weights.append(self.initializer(neurons[i+1],neurons[i])) # weight matrix between layer i and layer i+1\n",
        "            self.biases.append(self.initializer(neurons[i+1],1))\n",
        "            self.layer_activations.append(Activations[i]) # activations for each layer\n",
        "\n",
        "    def linear(self, z):\n",
        "        \"\"\"returns result of the linear activation function on input z.\"\"\"\n",
        "        return z\n",
        "\n",
        "    def linearPrime(self,z):\n",
        "        \"\"\"returns derivative of linear activation function applied on input z.\"\"\"\n",
        "        return np.ones_like(dz)\n",
        "\n",
        "    def ReLU(self,z):\n",
        "        \"\"\"returns result of the ReLU activation function on input z.\"\"\"\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def ReLUPrime(self,z):\n",
        "        \"\"\"returns derivative of ReLU activation function applied on input z.\"\"\"\n",
        "        return (z > 0).astype(np.float32)\n",
        "\n",
        "    def lossSE(self, predicted, actual):\n",
        "        \"\"\"Implementation of Squared-error loss function.\"\"\"\n",
        "        return 0.5*np.mean((predicted - actual)**2)\n",
        "\n",
        "    def lossCE(self, predicted, actual):\n",
        "        \"\"\"Implementation of Cross-Entropy loss function.\"\"\"\n",
        "        return -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines forward pass of the NN\n",
        "\n",
        "        Args:\n",
        "            x (array): input of size self.inputSize\n",
        "\n",
        "        Returns:\n",
        "            a (array): output of the forward pass\n",
        "            layer_dot_prod_z (list): layerwise intermediate outputs\n",
        "            layer_activations_a (list): layerwise activations\n",
        "        \"\"\"\n",
        "\n",
        "        layer_activations_a = [x] # store the outputs of activation\n",
        "        a=x  ## storing input as activation of zero-th layer\n",
        "        layer_dot_prod_z = []\n",
        "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = param[0], param[1]\n",
        "\n",
        "            #\n",
        "            z = np.dot(w,x) + b\n",
        "            #\n",
        "\n",
        "            # if self.layer_activations[i].lower()=='sigmoid':\n",
        "            #     a = self.sigmoid(z)\n",
        "            if self.layer_activations[i].lower()=='relu':\n",
        "                a = self.ReLU(z)\n",
        "            elif self.layer_activations[i].lower()=='linear':\n",
        "                a = self.linear(z)\n",
        "            # elif self.layer_activations[i].lower()=='softmax':\n",
        "            #     a = self.softmax(z)\n",
        "            layer_dot_prod_z.append(z)\n",
        "            layer_activations_a.append(a)\n",
        "        return a, layer_dot_prod_z, layer_activations_a\n",
        "\n",
        "    def backward(self, x, y, zs, activations):\n",
        "        \"\"\"backward pass to calculate gradient of loss w.r.t. NN parameters\n",
        "\n",
        "        Args:\n",
        "            x (array): input\n",
        "            y (array): ouputs from forward pass\n",
        "            zs (list): layerwise intermediate outputs from forward pass\n",
        "            activations (list): layerwise activations from forward pass\n",
        "\n",
        "        Returns:\n",
        "            [tuple]: gradients with respect to bias and weight parameters respectively\n",
        "        \"\"\"\n",
        "        n = zs[-1].shape[0]\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        for t in range(1, len(activations)):\n",
        "            if t == 1:\n",
        "                delta = (activations[-t] - y)\n",
        "                if np.isnan(activations[-t]).any():\n",
        "                    print(\"NaN detected in delta after backpropagation.\")\n",
        "                    return\n",
        "            else:\n",
        "                delta = np.dot(delta, self.weights[-t+1])\n",
        "                if np.isnan(delta).any():\n",
        "                    print(\"NaN detected in delta after backpropagation.\")\n",
        "                    return\n",
        "                \n",
        "            if self.layer_activations[-t].lower()=='sigmoid':\n",
        "                delta *= self.sigmoidPrime(zs[-t])\n",
        "            elif self.layer_activations[-t].lower()=='relu':\n",
        "                delta *= self.ReLUPrime(zs[-t])\n",
        "            elif self.layer_activations[-t].lower()=='softmax':\n",
        "                delta *= self.softmaxPrime(zs[-t], y)\n",
        "            else:\n",
        "                raise ValueError(\"Activation function not supported\")\n",
        "            grad_b[-t] = (np.sum(delta, axis=0, keepdims=True) / n).T\n",
        "            grad_w[-t] = (np.dot( activations[-t-1].T, delta) / n).T\n",
        "\n",
        "        return (grad_b, grad_w)\n",
        "\n",
        "    def update_parameters(self, grads, lr):\n",
        "        \"\"\"update the NN parameters using the gradients\n",
        "\n",
        "        Args:\n",
        "            grads (list): gradients obtained from backward pass\n",
        "            lr (float): learning rate of NN\n",
        "        \"\"\"\n",
        "\n",
        "        grad_b, grad_w = grads[0], grads[1]\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= lr * grad_w[i]\n",
        "            self.biases[i] -= lr * grad_b[i]\n",
        "\n",
        "    def copy_params(self):\n",
        "        \"\"\"Returns a copy of current NN parameters\"\"\"\n",
        "        weights = [w.copy() for w in self.weights]\n",
        "        biases = [b.copy() for b in self.biases]\n",
        "        return (weights, biases)\n",
        "\n",
        "    def error(self, X, Y, errors):\n",
        "        \"\"\"Appends loss to error list\"\"\"\n",
        "        y = np.squeeze(self.forward(X)[0])\n",
        "        errors.append(self.loss(y, Y))\n",
        "\n",
        "    def train(self, X, Y, lr = 1e-3, max_epochs = 1000, patience=5, batch_size = None,\n",
        "              n_classes=10, onehotencoded=False, loss_func='SE', Xval=None, Yval=None, verbose=True):\n",
        "\n",
        "        if onehotencoded:\n",
        "            # a method for creating one hot encoded labels\n",
        "            def onehotencoding(Y, n):\n",
        "                # one-hot encoding of class i is just the ith column of the identity\n",
        "                # matrix of size n where n is the total number of classes. so below\n",
        "                # code uses Y (a 1d array) for indexing into the identity matrix.\n",
        "\n",
        "                ### TODO: return one-hot encoding for Y\n",
        "                onehot = np.zeros((n, Y.shape[1]))\n",
        "                onehot[Y[0].astype(int), np.arange(Y.shape[1])] = 1\n",
        "                return onehot\n",
        "\n",
        "            Y = onehotencoding(Y, n_classes)\n",
        "            if Yval is not None:\n",
        "                Yval = onehotencoding(Yval, n_classes)\n",
        "\n",
        "        # Below code ensures that Y is 2-dimensional even when one-hot encoding is not\n",
        "        # performed, so our same code works for training NN for both tasks.\n",
        "        Y = np.expand_dims(Y,0) if len(Y.shape) == 1 else Y\n",
        "        Yval = np.expand_dims(Yval,0) if (Yval is not None and len(Yval.shape) == 1) else Yval\n",
        "\n",
        "        if loss_func == 'SE':\n",
        "            self.loss = self.lossSE\n",
        "        elif loss_func == 'CE':\n",
        "            self.loss = self.lossCE\n",
        "\n",
        "        train_errors=[]\n",
        "        # if Xval is not None:\n",
        "        #     val_errors=[]\n",
        "        val_errors=[]\n",
        "\n",
        "        i, j, v = 0, 0, np.inf     ## i -> epoch, j->patience, v ->best loss\n",
        "        best_params = self.copy_params()\n",
        "\n",
        "        if batch_size is not None:\n",
        "            if batch_size > len(X[0]):\n",
        "                raise ValueError(\"invalid mini-batch size. Must be smaller than dataset length\")\n",
        "        else:   ## if batch_size is not given\n",
        "            batch_size = len(X[0])\n",
        "\n",
        "        #LR control function\n",
        "        if isinstance(lr, (float, int)):# constant Ir arg is passed to train get_ir lambda x: 1r # we return a function object which returns the same ir at every epoc elif callable(lr): # function in arg is passed to train\n",
        "          get_lr =lambda X: lr # the function is then used to get in for a particular epoch else: raise ValueError('param Ir can only be a number or a scheduler function\")\n",
        "        elif callable(lr):\n",
        "          get_lr =lr\n",
        "        else:\n",
        "          raise ValueError('param lr can only be a number or a scheduler function')\n",
        "\n",
        "        while j < patience:\n",
        "\n",
        "            ### TODO: Implement the training algorithm with option for mini-batches\n",
        "            # Shuffle Dataset\n",
        "            num_samples = X.shape[1]\n",
        "            idx = np.random.permutation(num_samples)\n",
        "            X = X[:, idx]\n",
        "            Y = Y[:, idx]\n",
        "\n",
        "            #for every batch do\n",
        "            for batch_start in range(0, num_samples, batch_size):\n",
        "                batch_end = min(batch_start + batch_size, num_samples)\n",
        "                X_batch = X[:, batch_start:batch_end]\n",
        "                Y_batch = Y[:, batch_start:batch_end]\n",
        "\n",
        "                #do forward pass\n",
        "                y, zs, activations = self.forward(X_batch)\n",
        "\n",
        "                #compute gradients using backpropagation\n",
        "                grads = self.backward(X_batch, Y_batch, zs, activations)\n",
        "\n",
        "                #update parameters\n",
        "                self.update_parameters(grads, get_lr(i))\n",
        "\n",
        "            i += 1 # increment epoch count\n",
        "\n",
        "            self.error(X, Y, train_errors)   ### appending the loss to train_errors\n",
        "            if Xval is not None:\n",
        "                self.error(Xval, Yval, val_errors)    ## appending the val_loss to errors\n",
        "\n",
        "                if val_errors[-1] < v:\n",
        "                    j = 0 # reset patience counter\n",
        "                    v = val_errors[-1] # update best loss\n",
        "                    best_params = self.copy_params() # save params\n",
        "                else:\n",
        "                    j += 1 # increment patience counter\n",
        "\n",
        "            if verbose and i%5 == 0:\n",
        "                log = f\"Epoch {i}..............Loss on train = {train_errors[-1]}\"\n",
        "                if Xval is not None:\n",
        "                    log += f\", Loss on val = {val_errors[-1]}\"\n",
        "                print(log)\n",
        "\n",
        "            if i >= max_epochs:\n",
        "                break # stop if epoch threshold crossed\n",
        "\n",
        "        if Xval is not None:\n",
        "            if i >= max_epochs and verbose:\n",
        "                print(\"Reached Epoch Cap without convergence....Terminating\")\n",
        "            elif verbose:\n",
        "                print(\"Early Stopping .............. Returning best weights\")\n",
        "\n",
        "            self.weights, self.biases = best_params # reset to best params\n",
        "\n",
        "        if verbose:\n",
        "            x = np.arange(1, len(train_errors)+1)\n",
        "            plt.plot(x, train_errors, label=\"Loss on Train\")\n",
        "            if Xval is not None:\n",
        "                plt.plot(x, val_errors, label=\"Loss on Val\")\n",
        "            plt.legend()\n",
        "            plt.title(f\"{loss_func} - Learning Rate = {lr}\")\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            plt.show()\n",
        "        if Xval is not None:\n",
        "            return (train_errors, val_errors)\n",
        "        return train_errors\n",
        "\n",
        "    def predict(self, x):\n",
        "        y_pred, _, _ = self.forward(x)\n",
        "        return np.round(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAO4ISfMEv5h"
      },
      "outputs": [],
      "source": [
        "D_in, H1, H2, D_out = 784, 200, 100, 1\n",
        "neurons = [D_in, H1, H2, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['linear','linear','linear'] # activations in each layer (Note: the input layer does not have any activation)\n",
        "nn_bin = Neural_Network(neurons, activation_functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX1c_NUey3Qv",
        "outputId": "3bd3ff39-40d6-420c-955a-e5b4f8b662cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10132,)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_bin[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ahFB7zZRE1eb",
        "outputId": "8098e89e-01a1-45e3-9615-18e01856bdaa"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (100,200) and (784,10) not aligned: 200 (dim 1) != 784 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-2c3ea2c6dce4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_bin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_val_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-2b17b73b093b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, lr, max_epochs, patience, batch_size, n_classes, onehotencoded, loss_func, Xval, Yval, verbose)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m#do forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m#compute gradients using backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-2b17b73b093b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (100,200) and (784,10) not aligned: 200 (dim 1) != 784 (dim 0)"
          ]
        }
      ],
      "source": [
        "losses = nn_bin.train(X_train_bin, Y_train_bin, batch_size=10, lr=1e-2, max_epochs=50, Xval = X_val_bin, Yval = Y_val_bin, verbose= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDGhE1pE_0k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
