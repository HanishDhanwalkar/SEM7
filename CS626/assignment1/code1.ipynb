{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in f:\\python program\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in f:\\python program\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in f:\\python program\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\python program\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in f:\\python program\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in f:\\python program\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\annan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\annan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pprint, time\n",
    "import nltk  # pip install nltk\n",
    "\n",
    "# Download the required NLTK corpora\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Load the Brown corpus with universal POS tags\n",
    "pos_tagged_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# Convert to a list for easier manipulation and inspection\n",
    "tagged_corpus = list(pos_tagged_sentences)\n",
    "\n",
    "# Preprocessing function for tagged sentences\n",
    "def preprocess_tagged_sentences(tagged_corpus):\n",
    "    processed_sentences = []\n",
    "    for sentence in tagged_corpus:\n",
    "        modified_sentence = [('SOS', 'SOS')]\n",
    "        for word, tag in sentence:\n",
    "            if word == '.':\n",
    "                modified_sentence.append((word, 'EOS'))\n",
    "            else:\n",
    "                modified_sentence.append((word, tag))\n",
    "        if not (sentence and sentence[-1][0] == '.'):\n",
    "            modified_sentence.append(('EOS', 'EOS'))\n",
    "        processed_sentences.append(modified_sentence)\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = preprocess_tagged_sentences(tagged_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('SOS', 'SOS'),\n",
       "  ('The', 'DET'),\n",
       "  ('Fulton', 'NOUN'),\n",
       "  ('County', 'NOUN'),\n",
       "  ('Grand', 'ADJ'),\n",
       "  ('Jury', 'NOUN'),\n",
       "  ('said', 'VERB'),\n",
       "  ('Friday', 'NOUN'),\n",
       "  ('an', 'DET'),\n",
       "  ('investigation', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  (\"Atlanta's\", 'NOUN'),\n",
       "  ('recent', 'ADJ'),\n",
       "  ('primary', 'NOUN'),\n",
       "  ('election', 'NOUN'),\n",
       "  ('produced', 'VERB'),\n",
       "  ('``', '.'),\n",
       "  ('no', 'DET'),\n",
       "  ('evidence', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  ('that', 'ADP'),\n",
       "  ('any', 'DET'),\n",
       "  ('irregularities', 'NOUN'),\n",
       "  ('took', 'VERB'),\n",
       "  ('place', 'NOUN'),\n",
       "  ('.', 'EOS')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_HMM(train_data):\n",
    "    # Flatten the training dataset into a list of word-tag pairs\n",
    "    train_word_tag_pairs = [pair for sentence in train_data for pair in sentence]\n",
    "\n",
    "    # Extract unique tags and words\n",
    "    unique_tags = list({tag for _, tag in train_word_tag_pairs})\n",
    "    unique_words = list({word for word, _ in train_word_tag_pairs})\n",
    "\n",
    "    # Precompute tag and word-tag frequencies\n",
    "    tag_freq = {}\n",
    "    word_tag_freq = {}\n",
    "    for word, tag in train_word_tag_pairs:\n",
    "        if tag not in tag_freq:\n",
    "            tag_freq[tag] = 0\n",
    "        if (word, tag) not in word_tag_freq:\n",
    "            word_tag_freq[(word, tag)] = 0\n",
    "        tag_freq[tag] += 1\n",
    "        word_tag_freq[(word, tag)] += 1\n",
    "\n",
    "    # Precompute transition probabilities\n",
    "    tag_list = list(unique_tags)\n",
    "    tag_index = {tag: idx for idx, tag in enumerate(tag_list)}\n",
    "    transition_matrix = np.zeros((len(unique_tags), len(unique_tags)), dtype='float32')\n",
    "\n",
    "    prev_tag = train_word_tag_pairs[0][1]\n",
    "    for i in range(1, len(train_word_tag_pairs)):\n",
    "        current_tag = train_word_tag_pairs[i][1]\n",
    "        if prev_tag in tag_index and current_tag in tag_index:\n",
    "            transition_matrix[tag_index[prev_tag], tag_index[current_tag]] += 1\n",
    "        prev_tag = current_tag\n",
    "\n",
    "    for i in range(len(tag_list)):\n",
    "        total = np.sum(transition_matrix[i, :])\n",
    "        if total > 0:\n",
    "            transition_matrix[i, :] /= total\n",
    "\n",
    "    transition_df = pd.DataFrame(transition_matrix, columns=tag_list, index=tag_list)\n",
    "\n",
    "    # Observation probabilities\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    tag_index = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "\n",
    "    observation_matrix = np.zeros((len(unique_tags), len(unique_words)), dtype='float32')\n",
    "    for word, tag in train_word_tag_pairs:\n",
    "        if word in word_index and tag in tag_index:\n",
    "            observation_matrix[tag_index[tag], word_index[word]] += 1\n",
    "\n",
    "    for i in range(len(unique_tags)):\n",
    "        total = np.sum(observation_matrix[i, :])\n",
    "        if total > 0:\n",
    "            observation_matrix[i, :] /= total\n",
    "\n",
    "    observation_df = pd.DataFrame(observation_matrix, columns=unique_words, index=unique_tags)\n",
    "\n",
    "    return transition_df, observation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, transition_df, observation_df):\n",
    "    unique_tags = transition_df.index.tolist()\n",
    "    n = len(words)\n",
    "    viterbi = [{}]\n",
    "    backpointer = [{}]\n",
    "    total_tags = len(unique_tags)\n",
    "    smoothing = 1\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        transition_p = transition_df.loc['SOS', tag] \n",
    "        emission_p = observation_df.loc[tag, words[1]] if words[1] in observation_df.columns else 0.001\n",
    "        viterbi[0][tag] = transition_p * emission_p\n",
    "        backpointer[0][tag] = 'SOS'\n",
    "\n",
    "    for t in range(2, n):\n",
    "        viterbi.append({})\n",
    "        backpointer.append({})\n",
    "        for tag in unique_tags:\n",
    "            #laplace smoothing to take care of unseen words\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (viterbi[t-2][prev_tag] * ((transition_df.loc[prev_tag, tag] + smoothing) / (transition_df.loc[prev_tag].sum() + smoothing * total_tags)) * \n",
    "                (observation_df.loc[tag, words[t]] if words[t] in observation_df.columns else 0.001), prev_tag)\n",
    "                for prev_tag in unique_tags\n",
    "            )\n",
    "            viterbi[t-1][tag] = max_prob\n",
    "            backpointer[t-1][tag] = best_prev_tag\n",
    "\n",
    "    best_sequence = []\n",
    "    last_tag = max(viterbi[-1], key=viterbi[-1].get)\n",
    "    best_sequence.append(last_tag)\n",
    "\n",
    "    for t in range(n-1, 0, -1):\n",
    "        last_tag = backpointer[t-1][last_tag]\n",
    "        best_sequence.append(last_tag)\n",
    "\n",
    "    best_sequence.reverse()\n",
    "    return list(zip(words, best_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_viterbi_on_test_data(test_data, transition_df, observation_df):\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "    all_true_tags = []\n",
    "    all_predicted_tags = []\n",
    "    num_sentences = len(test_data)\n",
    "    print_interval = 2000\n",
    "\n",
    "    for i, sentence in enumerate(test_data):\n",
    "        words = [word for word, _ in sentence]\n",
    "        true_tags = [tag for _, tag in sentence]\n",
    "        tagged_seq = Viterbi(words, transition_df, observation_df)\n",
    "        predicted_tags = [tag for _, tag in tagged_seq]\n",
    "        correct_tags += sum(p == t for p, t in zip(predicted_tags, true_tags))\n",
    "        total_tags += len(true_tags)\n",
    "\n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_predicted_tags.extend(predicted_tags)\n",
    "\n",
    "        if (i + 1) % print_interval == 0 or (i + 1) == num_sentences:\n",
    "            print(f'Processed {i + 1}/{num_sentences} sentences')\n",
    "\n",
    "    accuracy = correct_tags / total_tags if total_tags > 0 else 0\n",
    "    return accuracy * 100, all_true_tags, all_predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def cross_validate_hmm(tagged_corpus, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    all_f1 = []\n",
    "    all_true_tags = []\n",
    "    all_predicted_tags = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(tagged_corpus)):\n",
    "        print(f\"Fold {fold + 1}\")\n",
    "        train_data = [tagged_corpus[i] for i in train_idx]\n",
    "        test_data = [tagged_corpus[i] for i in test_idx]\n",
    "\n",
    "        # Train HMM\n",
    "        transition_df, observation_df = train_HMM(train_data)\n",
    "\n",
    "        # Evaluate on test data\n",
    "        accuracy, true_tags, predicted_tags = evaluate_viterbi_on_test_data(test_data, transition_df, observation_df)\n",
    "\n",
    "        # Calculate precision, recall, f1-score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true_tags, predicted_tags, average=None, labels=sorted(set(true_tags)))\n",
    "        # Accumulate scores\n",
    "        all_precision.append(precision)\n",
    "        all_recall.append(recall)\n",
    "        all_f1.append(f1)\n",
    "\n",
    "        # Accumulate true and predicted tags for confusion matrix\n",
    "        all_true_tags.extend(true_tags)\n",
    "        all_predicted_tags.extend(predicted_tags)\n",
    "\n",
    "        print(f'Accuracy for fold {fold + 1}: {accuracy:.2f}%')\n",
    "        accuracies.append(accuracy)\n",
    "     \n",
    "    avg_precision = np.mean(all_precision, axis=0)\n",
    "    avg_recall = np.mean(all_recall, axis=0)\n",
    "    avg_f1 = np.mean(all_f1, axis=0)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    print(f'\\nAverage Accuracy over {n_splits} folds: {avg_accuracy:.2f}%')\n",
    "\n",
    "    labels = sorted(set(all_true_tags))  # Get all unique tags\n",
    "    conf_matrix = confusion_matrix(all_true_tags, all_predicted_tags, labels=labels)\n",
    "\n",
    "    # Convert to DataFrame for better readability\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "    # Calculate per-POS accuracy\n",
    "    per_pos_accuracy = {}\n",
    "    for i, tag in enumerate(labels):\n",
    "        true_positives = conf_matrix[i, i]\n",
    "        total_relevant = np.sum(conf_matrix[i, :])\n",
    "        per_pos_accuracy[tag] = true_positives / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "    # Calculate average precision, recall, F1-score\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "        all_true_tags, all_predicted_tags, average='macro'\n",
    "    )\n",
    "    \n",
    "    # Calculate F2 score (beta=2) and F0.5 score (beta=0.5)\n",
    "    _, _, f2_score, _ = precision_recall_fscore_support(\n",
    "        all_true_tags, all_predicted_tags, average='macro', beta=2\n",
    "    )\n",
    "\n",
    "    _, _, f0_5_score, _ = precision_recall_fscore_support(\n",
    "        all_true_tags, all_predicted_tags, average='macro', beta=0.5\n",
    "    )\n",
    "\n",
    "    return conf_matrix_df, per_pos_accuracy, avg_precision, avg_recall, avg_f1, overall_precision, overall_recall, overall_f1, f2_score, f0_5_score\n",
    "\n",
    "# Run 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Processed 2000/11468 sentences\n",
      "Processed 4000/11468 sentences\n",
      "Processed 6000/11468 sentences\n",
      "Processed 8000/11468 sentences\n",
      "Processed 10000/11468 sentences\n",
      "Processed 11468/11468 sentences\n",
      "Accuracy for fold 1: 94.36%\n",
      "Fold 2\n",
      "Processed 2000/11468 sentences\n",
      "Processed 4000/11468 sentences\n",
      "Processed 6000/11468 sentences\n",
      "Processed 8000/11468 sentences\n",
      "Processed 10000/11468 sentences\n",
      "Processed 11468/11468 sentences\n",
      "Accuracy for fold 2: 94.40%\n",
      "Fold 3\n",
      "Processed 2000/11468 sentences\n",
      "Processed 4000/11468 sentences\n",
      "Processed 6000/11468 sentences\n",
      "Processed 8000/11468 sentences\n",
      "Processed 10000/11468 sentences\n",
      "Processed 11468/11468 sentences\n",
      "Accuracy for fold 3: 94.45%\n",
      "Fold 4\n",
      "Processed 2000/11468 sentences\n",
      "Processed 4000/11468 sentences\n",
      "Processed 6000/11468 sentences\n",
      "Processed 8000/11468 sentences\n",
      "Processed 10000/11468 sentences\n",
      "Processed 11468/11468 sentences\n",
      "Accuracy for fold 4: 94.36%\n",
      "Fold 5\n",
      "Processed 2000/11468 sentences\n",
      "Processed 4000/11468 sentences\n",
      "Processed 6000/11468 sentences\n",
      "Processed 8000/11468 sentences\n",
      "Processed 10000/11468 sentences\n",
      "Processed 11468/11468 sentences\n",
      "Accuracy for fold 5: 94.27%\n",
      "\n",
      "Average Accuracy over 5 folds: 94.37%\n",
      "Average Confusion Matrix:\n",
      "           .    ADJ     ADP    ADV   CONJ     DET    EOS    NOUN    NUM   PRON  \\\n",
      ".     98132      0       0      0      0       0      0       0      0      0   \n",
      "ADJ      55  75537     125   3609      0    1485     61    1792      5     76   \n",
      "ADP      23    134  129270   1849    147      60      0      22      1     68   \n",
      "ADV      32   1636    2062  50282    102     514     11     272      0    143   \n",
      "CONJ      0      0       2     91  37992      20      0       0      0      0   \n",
      "DET       0      2    1624     14     97  133967      1       0      2   1208   \n",
      "EOS       0      0       0      0     32       0  57308       0      0      0   \n",
      "NOUN    567   6330     404    527      5    3148    289  253459    579   1235   \n",
      "NUM      24     74      21      0      0     398      8     319  13859     46   \n",
      "PRON      0      1     363      2      0     163      1       8      0  48765   \n",
      "PRT       4    305    1127     93      0      42      4     147      0     32   \n",
      "SOS       0      0       0      0      0       0      0       0      0      0   \n",
      "VERB    140   1008     912    128      1     920     28    6219      0     67   \n",
      "X        34     38      40      5      4      69     36     293      1     21   \n",
      "\n",
      "        PRT    SOS    VERB     X  \n",
      ".         0      0       0    87  \n",
      "ADJ      99     17     731   129  \n",
      "ADP   12951      4      36   201  \n",
      "ADV     865      9     257    54  \n",
      "CONJ      0      0       0    46  \n",
      "DET       1      0       1   102  \n",
      "EOS       0      0       0     0  \n",
      "NOUN    123    329    6493  2070  \n",
      "NUM       0      3      80    42  \n",
      "PRON      7      1       4    19  \n",
      "PRT   28006      6      34    29  \n",
      "SOS       0  57340       0     0  \n",
      "VERB     30     30  172753   514  \n",
      "X         4     40      36   765  \n",
      "Per-POS Accuracy:\n",
      " {'.': 0.9991142243354137, 'ADJ': 0.9022467481277099, 'ADP': 0.8929582913114958, 'ADV': 0.8940770639591742, 'CONJ': 0.9958323503971063, 'DET': 0.9777257168713828, 'EOS': 0.9994419253575165, 'NOUN': 0.9198027275564491, 'NUM': 0.9317601183272826, 'PRON': 0.988466372076053, 'PRT': 0.938884977706259, 'SOS': 1.0, 'VERB': 0.945296853625171, 'X': 0.551948051948052}\n",
      "\n",
      "Average Precision per POS tag:\n",
      " {'.': 0.9911177957041207, 'ADJ': 0.8880071520938475, 'ADP': 0.9508695315887413, 'ADV': 0.8883847051664958, 'CONJ': 0.9898944922526958, 'DET': 0.9515602975502366, 'EOS': 0.9923982776298585, 'NOUN': 0.9654435157603967, 'NUM': 0.9592996850564098, 'PRON': 0.9440358361413395, 'PRT': 0.6654043207225475, 'SOS': 0.9924023940796417, 'VERB': 0.9574771636806878, 'X': 0.189269988029319}\n",
      "\n",
      "Average Recall per POS tag:\n",
      " {'.': 0.999115119585072, 'ADJ': 0.9022699960832249, 'ADP': 0.8929555024586593, 'ADV': 0.8940784964826071, 'CONJ': 0.9958304639701645, 'DET': 0.9777517666725275, 'EOS': 0.9994419253575166, 'NOUN': 0.9198073920293452, 'NUM': 0.9317325415938267, 'PRON': 0.9884584776093295, 'PRT': 0.9389216349075887, 'SOS': 1.0, 'VERB': 0.9452974568515818, 'X': 0.55511905965363}\n",
      "\n",
      "Average F1-Score per POS tag:\n",
      " {'.': 0.9951003295949252, 'ADJ': 0.8950789493876184, 'ADP': 0.9210016859729316, 'ADV': 0.8912187519226535, 'CONJ': 0.9928507387071146, 'DET': 0.9644765482626465, 'EOS': 0.9959075119145598, 'NOUN': 0.9420727477467615, 'NUM': 0.9453097685350815, 'PRON': 0.9657288338911615, 'PRT': 0.7788290897961104, 'SOS': 0.9961866332514286, 'VERB': 0.9513482113174525, 'X': 0.2811843348086358}\n",
      "\n",
      "Overall Precision: 0.8803\n",
      "Overall Recall: 0.9241\n",
      "Overall F1-Score: 0.8940\n",
      "Overall F2-Score: 0.9083\n",
      "Overall F0.5-Score: 0.8849\n"
     ]
    }
   ],
   "source": [
    "conf_matrix_df, per_pos_accuracy, avg_precision, avg_recall, avg_f1, overall_precision, overall_recall, overall_f1, f2_score, f0_5_score =cross_validate_hmm(processed_corpus)\n",
    "print(\"Average Confusion Matrix:\\n\", conf_matrix_df)\n",
    "print(\"Per-POS Accuracy:\\n\", per_pos_accuracy)\n",
    "print(\"\\nAverage Precision per POS tag:\\n\", dict(zip(conf_matrix_df.index, avg_precision)))\n",
    "print(\"\\nAverage Recall per POS tag:\\n\", dict(zip(conf_matrix_df.index, avg_recall)))\n",
    "print(\"\\nAverage F1-Score per POS tag:\\n\", dict(zip(conf_matrix_df.index, avg_f1)))\n",
    "print(f\"\\nOverall Precision: {overall_precision:.4f}\")\n",
    "print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "print(f\"Overall F1-Score: {overall_f1:.4f}\")\n",
    "print(f\"Overall F2-Score: {f2_score:.4f}\")\n",
    "print(f\"Overall F0.5-Score: {f0_5_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_df, observation_df = train_HMM(processed_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming transition_df and observation_df are computed as before\n",
    "# Save the transition and observation matrices to CSV files\n",
    "transition_df.to_csv('transition_matrix.csv')\n",
    "observation_df.to_csv('observation_matrix.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
