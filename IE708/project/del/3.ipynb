{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Compute Expert’s Feature Expectations \n",
    "For each trajectory, compute the expected outcomes using the feature map (\n",
    "\n",
    "Example: The feature map might capture tumor reduction and side effects:\n",
    "\n",
    "$\\phi(h_t, a_t)$ =[expected tumor size, side effect severity]\n",
    "\n",
    "Aggregate these over the trajectories to compute the expert's feature expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Initialize Random Reward Weights ($w_0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the Initial Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Iteratively Improve the Reward Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_map(history, action):\n",
    "    \"\"\"\n",
    "    Computes the feature map φ(h_t, a_t) = E[Y_{t+1}[a_t] | h_t]\n",
    "    Placeholder: You need to define your own feature computation.\n",
    "    \"\"\"\n",
    "    # Example: Placeholder for feature computation\n",
    "    return np.random.rand(3)  # Example: 3-dimensional feature vector\n",
    "\n",
    "\n",
    "def compute_feature_expectations(data, policy, feature_map_fn):\n",
    "    \"\"\"\n",
    "    Computes feature expectations μ^π for a given policy π.\n",
    "    :param data: Dataset of trajectories.\n",
    "    :param policy: Current policy.\n",
    "    :param feature_map_fn: Function to compute feature map φ(h_t, a_t).\n",
    "    :return: Feature expectations μ^π.\n",
    "    \"\"\"\n",
    "    feature_expectations = np.zeros(feature_map_fn(None, None).shape)\n",
    "    for trajectory in data:\n",
    "        for history, action in trajectory:\n",
    "            feature_expectations += feature_map_fn(history, action) * policy(history, action)\n",
    "    return feature_expectations / len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orthogonal_projection(mu_pi_e, mu_bar_prev, mu_pi_k):\n",
    "    \"\"\"\n",
    "    Orthogonally project μ^π_E onto the line through μ̄_{k-1} and μ^π_k.\n",
    "    :param mu_pi_e: Expert feature expectations.\n",
    "    :param mu_bar_prev: Previous projection μ̄_{k-1}.\n",
    "    :param mu_pi_k: Current feature expectations μ^π_k.\n",
    "    :return: Updated μ̄_k.\n",
    "    \"\"\"\n",
    "    direction = mu_pi_k - mu_bar_prev\n",
    "    t = np.dot(mu_pi_e - mu_bar_prev, direction) / np.dot(direction, direction)\n",
    "    return mu_bar_prev + t * direction, t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_max_margin_cirl(data, feature_map_fn, max_iterations, epsilon):\n",
    "    \"\"\"\n",
    "    Implements the Batch Max-Margin CIRL algorithm.\n",
    "    :param data: Batch dataset D of trajectories.\n",
    "    :param feature_map_fn: Function to compute feature map φ(h_t, a_t).\n",
    "    :param max_iterations: Maximum number of iterations.\n",
    "    :param epsilon: Convergence threshold.\n",
    "    :return: Final reward function, policies Π, and feature expectations Δ.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    mu_pi_e = compute_feature_expectations(data, lambda h, a: 1, feature_map_fn)  # Expert's feature expectations\n",
    "    w = np.random.rand(mu_pi_e.shape[0])  # Random initial reward weights\n",
    "    pi_0 = lambda h, a: 1  # Initial policy (Uniform policy for example)\n",
    "    mu_pi_0 = compute_feature_expectations(data, pi_0, feature_map_fn)\n",
    "    policies = [pi_0]\n",
    "    feature_expectations = [mu_pi_0]\n",
    "    mu_bar = mu_pi_0  # μ̄_0\n",
    "\n",
    "    for k in range(1, max_iterations + 1):\n",
    "        # Compute reward function R_k = w_k · φ(h, a)\n",
    "        R_k = lambda h, a: np.dot(w, feature_map_fn(h, a))\n",
    "        \n",
    "        # Derive optimal policy π_k based on R_k (e.g., via RL)\n",
    "        pi_k = lambda h, a: np.random.choice([0, 1])  # Placeholder policy\n",
    "        mu_pi_k = compute_feature_expectations(data, pi_k, feature_map_fn)\n",
    "\n",
    "        # Update policies and feature expectations\n",
    "        policies.append(pi_k)\n",
    "        feature_expectations.append(mu_pi_k)\n",
    "\n",
    "        # Orthogonal projection\n",
    "        mu_bar, t = orthogonal_projection(mu_pi_e, mu_bar, mu_pi_k)\n",
    "\n",
    "        # Update reward weights\n",
    "        w = mu_pi_e - mu_bar\n",
    "\n",
    "        # Check for convergence\n",
    "        if t < epsilon:\n",
    "            break\n",
    "\n",
    "    # Find optimal reward\n",
    "    K = np.argmin([np.linalg.norm(mu_pi_e - mu) for mu in feature_expectations])\n",
    "    final_reward = lambda h, a: np.dot(w, feature_map_fn(h, a))\n",
    "\n",
    "    return final_reward, policies, feature_expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset: List of trajectories (history, action)\n",
    "dataset = [\n",
    "    [(np.array([1, 2, 3]), 0), (np.array([2, 3, 4]), 1)],\n",
    "    [(np.array([3, 4, 5]), 1), (np.array([4, 5, 6]), 0)],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectories: [[(array([1, 2, 3]), 0), (array([2, 3, 4]), 1)], [(array([3, 4, 5]), 1), (array([4, 5, 6]), 0)]]\n"
     ]
    }
   ],
   "source": [
    "print(f'trajectories: {dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reward Function Weights: <function batch_max_margin_cirl.<locals>.<lambda> at 0x000001E24F575620>\n",
      "Policies: [<function batch_max_margin_cirl.<locals>.<lambda> at 0x000001E24F576200>, <function batch_max_margin_cirl.<locals>.<lambda> at 0x000001E24F5762A0>, <function batch_max_margin_cirl.<locals>.<lambda> at 0x000001E24F574860>]\n",
      "Feature Expectations: [array([1.07027557, 1.06531077, 0.44895422]), array([0.35799618, 0.12431546, 0.47416783]), array([0.19498702, 0.16758535, 0.41970448])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Run Batch Max-Margin CIRL\n",
    "reward, policies, feature_expectations = batch_max_margin_cirl(\n",
    "    data=dataset,\n",
    "    feature_map_fn=feature_map,\n",
    "    max_iterations=10,\n",
    "    epsilon=1e-3\n",
    ")\n",
    "\n",
    "print(\"Final Reward Function Weights:\", reward)\n",
    "print(\"Policies:\", policies)\n",
    "print(\"Feature Expectations:\", feature_expectations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
