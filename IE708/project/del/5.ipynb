{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Reward Function Weights: [0. 0.]\n",
      "Set of Feature Expectations: [array([0.5, 0. ]), array([0.5, 0. ])]\n",
      "Set of Policies: [array([0.14106641, 0.54531719]), array([0.11046977, 0.70917225])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_feature_expectations(policy, feature_map, dataset):\n",
    "    \"\"\"Compute the feature expectations for a given policy.\"\"\"\n",
    "    feature_expectations = np.zeros_like(feature_map(dataset[0][0][0], dataset[0][0][1]))\n",
    "    for trajectory in dataset:\n",
    "        for h, a in trajectory:\n",
    "            feature_expectations += feature_map(h, a)\n",
    "    return feature_expectations / len(dataset)\n",
    "\n",
    "def compute_optimal_policy(reward_weights, feature_map, dataset):\n",
    "    \"\"\"\n",
    "    Compute the optimal policy for a given reward function.\n",
    "    Placeholder for RL optimization routine.\n",
    "    \"\"\"\n",
    "    # Replace with RL solver to compute optimal policy π for R = w · φ\n",
    "    return np.random.random(len(reward_weights))  # Placeholder\n",
    "\n",
    "def orthogonal_projection(mu_e, mu_prev, mu_current):\n",
    "    \"\"\"Orthogonally project μ^π_E onto the line through μ̄_k−1 and μ^π_k.\"\"\"\n",
    "    diff = mu_current - mu_prev\n",
    "    denom = np.dot(diff, diff)\n",
    "    \n",
    "    if denom == 0:\n",
    "        # If the difference is zero, return the previous projection\n",
    "        return mu_prev\n",
    "    \n",
    "    projection = ((np.dot(mu_e - mu_prev, diff) / denom) * diff) + mu_prev\n",
    "    return projection\n",
    "\n",
    "\n",
    "def batch_max_margin_cirl(dataset, feature_map, max_iterations=100, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Batch, Max-Margin CIRL implementation.\n",
    "\n",
    "    Args:\n",
    "        dataset: A list of trajectories where each trajectory is a list of (h, a) tuples.\n",
    "        feature_map: A function φ(h, a) to compute feature expectations.\n",
    "        max_iterations: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "    \n",
    "    Returns:\n",
    "        R̃: Reward function weights.\n",
    "        Δ: Set of feature expectations.\n",
    "        Π: Set of policies.\n",
    "    \"\"\"\n",
    "    mu_e = compute_feature_expectations(None, feature_map, dataset)  # Expert feature expectations\n",
    "    w_0 = np.random.random(len(mu_e))  # Random initial reward weights\n",
    "    pi_0 = compute_optimal_policy(w_0, feature_map, dataset)\n",
    "    mu_pi_0 = compute_feature_expectations(pi_0, feature_map, dataset)\n",
    "    \n",
    "    Pi = [pi_0]\n",
    "    Delta = [mu_pi_0]\n",
    "    mu_bar = mu_pi_0\n",
    "\n",
    "    for k in range(1, max_iterations + 1):\n",
    "        w_k = mu_e - mu_bar\n",
    "        pi_k = compute_optimal_policy(w_k, feature_map, dataset)\n",
    "        mu_pi_k = compute_feature_expectations(pi_k, feature_map, dataset)\n",
    "\n",
    "        Pi.append(pi_k)\n",
    "        Delta.append(mu_pi_k)\n",
    "\n",
    "        mu_bar = orthogonal_projection(mu_e, mu_bar, mu_pi_k)\n",
    "        t = np.linalg.norm(mu_e - mu_bar, ord=2)\n",
    "\n",
    "        if t < epsilon:\n",
    "            break\n",
    "\n",
    "    # Find the best policy in the set Π\n",
    "    K = np.argmin([np.linalg.norm(mu_e - mu_pi, ord=2) for mu_pi in Delta])\n",
    "    R_tilde = w_k  # Final reward function weights\n",
    "\n",
    "    return R_tilde, Delta, Pi\n",
    "\n",
    "# Example Usage\n",
    "def feature_map(h, a):\n",
    "    \"\"\"Example feature map function φ(h, a).\"\"\"\n",
    "    h = np.array(h)  # Ensure `h` is a numpy array\n",
    "    return h * a  # Example feature map, adjust as needed.\n",
    "\n",
    "# Example dataset (list of trajectories with (state, action) pairs)\n",
    "# Ensure states (`h`) are numpy arrays and actions (`a`) are scalars\n",
    "dataset = [\n",
    "    [(np.array([1, 0]), 1), (np.array([0, 1]), 0)], \n",
    "    [(np.array([1, 1]), 0), (np.array([0, 0]), 1)]\n",
    "]\n",
    "\n",
    "R_tilde, Delta, Pi = batch_max_margin_cirl(dataset, feature_map)\n",
    "print(\"Final Reward Function Weights:\", R_tilde)\n",
    "print(\"Set of Feature Expectations:\", Delta)\n",
    "print(\"Set of Policies:\", Pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
